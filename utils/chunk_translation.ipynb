{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #importing libraries necessary for the code\n",
    "# from langchain.schema.runnable import RunnableMap\n",
    "# from functools import partial\n",
    "# from langchain_core.runnables.base import RunnableEach\n",
    "# from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#working on the initialzation of the model\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "#workign on the gpu\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "#initalizing the tokenizer and the LLM\n",
    "model_id = \"model starcoder/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "#will need the pipeline to facilitate later the integration with Langchain\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=250, return_full_text=False, device=device)\n",
    "\n",
    "#integrating the LLM with langchain\n",
    "hf = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cyrin\\Documents\\SUPCOM\\tutore2\\Code-Converter\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#working on the initialzation of the model\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "#workign on the gpu\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "#initalizing the tokenizer and the LLM\n",
    "model_id = \"model starcoder/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16,)\n",
    "\n",
    "#will need the pipeline to facilitate later the integration with Langchain\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=250, return_full_text=False, device=device)\n",
    "\n",
    "#integrating the LLM with langchain\n",
    "hf = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized configuration class <class 'transformers_modules.configuration_codet5p_bimodal.CodeT5pBimodalConfig'> for this kind of AutoModel: AutoModelForCausalLM.\nModel type should be one of BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CohereConfig, CpmAntConfig, CTRLConfig, Data2VecTextConfig, DbrxConfig, ElectraConfig, ErnieConfig, FalconConfig, FalconMambaConfig, FuyuConfig, GemmaConfig, Gemma2Config, GitConfig, GlmConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GraniteConfig, GraniteMoeConfig, JambaConfig, JetMoeConfig, LlamaConfig, MambaConfig, Mamba2Config, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MistralConfig, MixtralConfig, MllamaConfig, MoshiConfig, MptConfig, MusicgenConfig, MusicgenMelodyConfig, MvpConfig, NemotronConfig, OlmoConfig, OlmoeConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, Phi3Config, PhimoeConfig, PLBartConfig, ProphetNetConfig, QDQBertConfig, Qwen2Config, Qwen2MoeConfig, RecurrentGemmaConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, Speech2Text2Config, StableLmConfig, Starcoder2Config, TransfoXLConfig, TrOCRConfig, WhisperConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, ZambaConfig.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel code t5/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     12\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id)\n\u001b[1;32m---> 13\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m#will need the pipeline to facilitate later the integration with Langchain\u001b[39;00m\n\u001b[0;32m     16\u001b[0m pipe \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m, return_full_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\cyrin\\Documents\\SUPCOM\\tutore2\\Code-Converter\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:567\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m    564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    565\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    566\u001b[0m     )\n\u001b[1;32m--> 567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    570\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Unrecognized configuration class <class 'transformers_modules.configuration_codet5p_bimodal.CodeT5pBimodalConfig'> for this kind of AutoModel: AutoModelForCausalLM.\nModel type should be one of BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CohereConfig, CpmAntConfig, CTRLConfig, Data2VecTextConfig, DbrxConfig, ElectraConfig, ErnieConfig, FalconConfig, FalconMambaConfig, FuyuConfig, GemmaConfig, Gemma2Config, GitConfig, GlmConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GraniteConfig, GraniteMoeConfig, JambaConfig, JetMoeConfig, LlamaConfig, MambaConfig, Mamba2Config, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MistralConfig, MixtralConfig, MllamaConfig, MoshiConfig, MptConfig, MusicgenConfig, MusicgenMelodyConfig, MvpConfig, NemotronConfig, OlmoConfig, OlmoeConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, Phi3Config, PhimoeConfig, PLBartConfig, ProphetNetConfig, QDQBertConfig, Qwen2Config, Qwen2MoeConfig, RecurrentGemmaConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, Speech2Text2Config, StableLmConfig, Starcoder2Config, TransfoXLConfig, TrOCRConfig, WhisperConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, ZambaConfig."
     ]
    }
   ],
   "source": [
    "#working on the initialzation of the model\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "#workign on the gpu\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "#initalizing the tokenizer and the LLM\n",
    "model_id = \"model code t5/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "#will need the pipeline to facilitate later the integration with Langchain\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=150, return_full_text=False, device=device)\n",
    "\n",
    "#integrating the LLM with langchain\n",
    "hf = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#working on the initialzation of the model\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "#workign on the gpu\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "#initalizing the tokenizer and the LLM\n",
    "#model_id = \"meta-llama/Llama-3.2-3B\"\n",
    "model_id = \"llama3/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "#will need the pipeline to facilitate later the integration with Langchain\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=150, return_full_text=False, device=device)\n",
    "\n",
    "#integrating the LLM with langchain\n",
    "hf = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt version 1\n",
    "#organizing the prompt\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = (\"You are instructed to convert/translate/give the equivalent of a code snippet from one programming language to another.\"\n",
    "            \"Read the description provided to you to help you understand better the functionnality of the code.\"\n",
    "            \"The description is a list of 4 elements each answering a question needed to help you understand the code\"\n",
    "            \"{description}\"\n",
    "            \"Give the equivalent of the code to Java\"\n",
    "            \"Here is the code you need to translate from Python to Java:\"\n",
    "            \"{code}\"  \n",
    "        )\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt verison 2\n",
    "#organizing the prompt\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = (\"You are a programming assistant skilled in translating code between programming languages. Your task is to convert Python code into Java based on the provided description and code snippet. \"\n",
    "            \"Maintain functionality, ensure idiomatic usage for the target language, and include comments explaining any key changes during the translation. \"\n",
    "            \"Below is the input:\"\n",
    "\n",
    "            \"### Description: \\n\"\n",
    "            \"{description}\"\n",
    "            \"### Python Code: \\n\"\n",
    "            \"{code}\"  \n",
    "\n",
    "            \"Provide the equivalent Java implementation.\"\n",
    "        )\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt verison 3\n",
    "#organizing the prompt\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = (\n",
    "    \"You are an expert code translator with deep knowledge of programming paradigms, syntax, and best practices in multiple programming languages. \"\n",
    "    \"Your task is to accurately translate a Python code snippet into Java. Adhere to the following guidelines:\"\n",
    "    \"1. Accuracy: Ensure the functionality of the Python code is preserved in the Java code. \"\n",
    "    \"2. Idiomacy: Write the Java code using idiomatic constructs typical of Java, such as proper use of classes, methods, type annotations, and exception handling. \"\n",
    "    \"3. Optimization: Optimize the Java code for readability and performance while maintaining its core functionality. \"\n",
    "    \"4. Comments: Include concise comments to explain any differences between Python and Java implementations, especially for constructs that do not directly map between the two languages. \"\n",
    "    \"5. Error Handling: Ensure that the Java code includes appropriate error handling if applicable. \"\n",
    "    \"6. Code Structure: Organize the Java code logically, adhering to Java conventions such as proper use of public static void main(String[] args) for executable programs. \"\n",
    "    \n",
    "    \"### Input Description: \"\n",
    "    \"A brief description of the code's purpose and functionality is provided to give context. Use this to guide the translation process.\"\n",
    "    \"{description} \"\n",
    "    \"### Input Python Code: \"\n",
    "    \"A Python code snippet is provided for translation.\"\n",
    "    \"{code} \"\n",
    "\n",
    "    \"### Output: \"\n",
    "    \"Provide the equivalent Java code snippet, maintaining structure, clarity, and function. Include explanatory comments where necessary.\"\n",
    "\n",
    "    \"Translate the given Python code based on the provided description.\"\n",
    "\n",
    "        )\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "public class Rectangle {\n",
      "\n",
      "public static void main(String[] args) {\n",
      "\n",
      "double length = Double.parseDouble(input(\"Enter the length of the rectangle: \"));\n",
      "double width = Double.parseDouble(input(\"Enter the width of the rectangle: \"));\n",
      "double area = calculate_area(length, width);\n",
      "print(f\"The area of the rectangle is: {area}\");\n",
      "}\n",
      "\n",
      "public static double calculate_area(double length, double width) {\n",
      "return length * width;\n",
      "}\n",
      "\n",
      "public static void print(String message) {\n",
      "System.out.println(message);\n",
      "}\n",
      "\n",
      "public static String input(String message) {\n",
      "System.out.print(message);\n",
      "return scanner.nextLine();\n",
      "}\n",
      "}\n",
      "\n",
      "### Explanation:\n",
      "\n",
      "The Python code snippet is translated into Java code using the following guidelines:\n",
      "\n",
      "• The calculate_area() function is translated into a public static double calculate_area(double length, double width) method.\n",
      "• The print() function is translated into a public static void print(String message) method.\n",
      "• The input() function is translated into a public static String input(String message) method.\n"
     ]
    }
   ],
   "source": [
    "description = (\"1. No, it does not use any libraries.\"\n",
    "\"2. Yes, it contains functions. The functions are calculate_area() and print().\"\n",
    "\"3. The number of inputs and outputs are 2 and 1 respectively.\"\n",
    "\"4. Yes, it contains loops (for, while) and if-statements.\")\n",
    "\n",
    "code = '''\n",
    "def calculate_area(length, width):\n",
    "    return length * width\n",
    "\n",
    "length = float(input(\"Enter the length of the rectangle: \"))\n",
    "width = float(input(\"Enter the width of the rectangle: \"))\n",
    "area = calculate_area(length, width)\n",
    "print(f\"The area of the rectangle is: {area}\"))\n",
    "    '''\n",
    "\n",
    "chain = prompt | hf \n",
    "#how to get the invoke to understand the code and the description both?\n",
    "descriptions = chain.invoke({'description':description, 'code':code})\n",
    "print(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
